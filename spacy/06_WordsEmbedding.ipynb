{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4345a50-e02d-4eca-a5a2-c0ef574f1342",
   "metadata": {},
   "source": [
    "# Explorando Word Embeddings com spaCy\n",
    "\n",
    "Neste notebook, vamos explorar o conceito de Word Embeddings, sua importância no Processamento de Linguagem Natural (PLN), e como podemos trabalhar com eles usando a biblioteca spaCy.\n",
    "\n",
    "### Objetivos:\n",
    "- Compreender o que são Word Embeddings e como funcionam.\n",
    "- Explorar diferentes tipos de embeddings.\n",
    "- Aprender a extrair, comparar e aplicar embeddings usando spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58275d1-3de7-4410-9f93-0a74facf4f73",
   "metadata": {},
   "source": [
    "# O que são Word Embeddings?\n",
    "\n",
    "Word Embeddings são representações vetoriais de palavras que capturam o significado semântico de uma palavra em um espaço contínuo. Ao contrário de representações esparsas, como Bag-of-Words, os embeddings mapeiam palavras para vetores densos de dimensões reduzidas, preservando relações semânticas.\n",
    "\n",
    "Modelos populares de Word Embeddings incluem:\n",
    "- **Word2Vec**: Um modelo baseado em redes neurais que usa o contexto das palavras para aprender suas representações.\n",
    "- **GloVe (Global Vectors for Word Representation)**: Um modelo que combina estatísticas globais de coocorrência de palavras para aprender representações.\n",
    "- **FastText**: Uma variação do Word2Vec que leva em conta subpalavras, o que melhora a representação de palavras raras ou morfologicamente ricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e89728-e72a-413d-aba6-117b5ee83bf9",
   "metadata": {},
   "source": [
    "## Trabalhando com Word Embeddings no spaCy\n",
    "\n",
    "O spaCy fornece modelos pré-treinados que incluem vetores de palavras (Word Embeddings). Esses vetores são acessíveis diretamente, e você pode usá-los para tarefas como comparação de similaridade entre palavras e frases.\n",
    "\n",
    "Vamos explorar como carregar um modelo de spaCy e usar seus embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6af016-c6b2-44f9-aa79-bfd2413f454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 350 palavras.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "\n",
    "print(f\"Tamanho do vocabulário: {len(nlp.vocab)} palavras.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d940560-63c9-46e4-bcbd-6882ed4b649e",
   "metadata": {},
   "source": [
    "## Explorando Vetores de Palavras\n",
    "\n",
    "Agora que carregamos o modelo, podemos explorar os vetores de palavras. Cada palavra no vocabulário do modelo possui um vetor associado, que representa sua posição no espaço vetorial.\n",
    "\n",
    "Vamos visualizar o vetor de uma palavra e entender o que ele representa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f90f310-e5cc-49b0-8e0c-9c215de3cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"homen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53327aec-0d58-4013-af57-9b4f4de406d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[word].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28b7417a-ac6c-42ae-98ed-2d58ce7e6271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor para a palavra 'homen':\n",
      "[-5.4599e-01 -1.6861e-01  1.5195e+00 -1.3087e+00 -1.1700e-01  1.1732e-01\n",
      " -7.9616e-01  7.5680e-01 -6.3276e-01  5.5164e-01 -1.6963e-01 -4.6776e-01\n",
      " -5.5000e-01 -1.0419e+00 -2.3745e-01  3.8617e-01 -1.4881e-02 -8.2770e-01\n",
      " -8.8004e-01 -3.4084e-01 -1.8027e+00 -3.1811e-01 -3.9425e-01  6.2820e-01\n",
      " -2.2139e+00  1.0263e+00 -1.3429e-02  5.4489e-01  2.2987e+00  8.6533e-01\n",
      " -5.0896e-02  3.0199e+00 -1.4524e+00  1.4374e+00  8.9107e-01  9.4465e-01\n",
      " -1.6955e-01 -1.4726e+00  1.5798e+00 -1.1678e+00 -1.4371e+00 -7.0871e-01\n",
      "  1.2503e+00  1.9593e+00 -1.1204e-01 -7.0902e-02 -1.1197e+00  1.2381e+00\n",
      "  2.4771e+00  4.1677e-02 -1.2796e+00  1.3167e-01  1.4707e+00 -1.1925e+00\n",
      " -8.5987e-01  3.3947e-01 -5.3827e-01 -2.6641e+00 -8.8770e-01  2.2820e+00\n",
      "  1.6084e+00  7.9834e-01  2.2490e+00  1.6650e+00 -1.8591e+00  4.4785e-01\n",
      " -1.3454e-01  4.8996e-02 -1.0863e+00 -7.5397e-01 -3.4468e-01 -1.6346e+00\n",
      "  4.4565e-01 -4.0422e-03  2.2581e+00 -8.6084e-01  1.5066e+00  1.8598e-01\n",
      " -2.4338e+00 -5.3052e-01 -6.8888e-01 -4.9156e-01 -2.0736e-01  6.4413e-02\n",
      " -1.3462e+00  2.2232e+00  1.7168e+00 -1.1856e+00 -9.3343e-01  1.5447e+00\n",
      " -1.3811e+00 -5.7961e-01  2.3140e+00 -1.0550e+00 -7.0072e-01 -4.5848e-01\n",
      "  4.1724e+00 -1.0384e+00 -1.5745e+00  5.7896e-01  6.3719e-01 -1.1843e+00\n",
      "  6.7399e-01 -6.3074e-01  5.4851e-01  2.4581e-01  1.0739e+00  6.4150e-01\n",
      "  1.3017e+00 -4.3210e-01 -1.1397e+00  1.7062e+00  1.6989e-01  1.7552e+00\n",
      " -2.2006e+00  2.3247e-01 -1.7071e+00 -4.3891e-01 -1.4218e+00 -4.3518e-02\n",
      " -1.3565e+00 -1.1775e+00  2.2077e+00 -5.7820e-02 -2.7471e-01  1.9083e+00\n",
      " -5.0512e-02  1.7201e+00 -4.4100e-01  5.0135e-01 -5.5573e-01 -1.2347e+00\n",
      " -1.4786e+00 -5.6267e-01 -1.9126e-01  4.7188e-01  8.1765e-01  2.0460e+00\n",
      " -4.2769e-01 -1.7790e+00 -1.9305e+00 -1.5518e+00  1.5677e+00 -1.0479e+00\n",
      "  1.4890e+00  1.1349e+00  5.9465e-01  1.5752e+00 -7.5611e-01 -1.6098e+00\n",
      "  3.8228e-01 -3.8066e-01 -1.2923e-01 -5.7453e-01  1.1614e+00  1.7438e-01\n",
      "  1.5451e+00 -1.9911e+00 -1.5050e+00  4.4174e-01  1.3952e+00 -3.9940e+00\n",
      " -8.2236e-01 -1.3505e+00 -5.4060e-02  7.3459e-02 -3.4052e-01  2.8285e-01\n",
      "  3.9541e-01  1.4560e+00 -3.2778e-01 -3.2936e-01  6.9439e-01  1.4384e+00\n",
      "  6.4131e-01 -9.3648e-01  3.6688e-01  4.3826e-01  1.3651e+00 -4.2606e-01\n",
      " -2.0163e+00  1.4541e+00  2.5173e+00  7.6221e-01  7.7062e-01 -1.2806e-01\n",
      "  1.3188e+00 -1.5585e+00 -8.0360e-03 -9.4908e-01 -2.5711e+00  9.9632e-01\n",
      "  1.7600e+00 -1.4607e+00 -1.1606e+00 -5.9278e-01 -1.4379e+00 -2.1052e+00\n",
      "  3.2278e-02  2.2889e+00  1.0688e-01 -1.9980e+00 -6.2730e-01 -2.0162e-01\n",
      " -2.8061e-02 -1.1288e-01  2.1561e+00 -3.6313e-01  2.1257e+00  1.4781e+00\n",
      " -4.6932e-01  1.5586e+00  2.2366e-01  5.3610e-01 -1.4019e+00  2.9026e+00\n",
      " -2.0348e-01 -5.0017e-01  1.1308e+00  5.5110e-01  1.9815e-01  7.4785e-01\n",
      "  4.2989e-01 -7.3322e-01  6.1399e-01 -1.6603e+00 -1.2052e+00  1.3029e+00\n",
      " -1.7526e-02  4.0570e-01  3.2464e-01 -5.9962e-01  6.0639e-01  1.3471e-01\n",
      "  2.5876e+00  2.1528e+00 -9.3278e-01 -5.3469e-01 -1.4083e+00 -7.9378e-01\n",
      " -6.2710e-01  2.0393e+00  3.4261e-01 -4.2674e-01  2.1562e-01 -2.3201e+00\n",
      "  1.0584e+00 -1.2001e+00  1.2901e+00 -1.4535e+00  2.4700e-01  1.2803e+00\n",
      "  2.7709e+00  3.5892e-01  1.5068e+00  5.0938e-01  7.1463e-01  2.7489e+00\n",
      "  5.1497e-01  4.9283e-01 -1.2610e+00 -2.1362e-01 -1.9332e-01  3.3286e+00\n",
      " -5.6444e-01 -5.5632e-01  1.0187e+00  8.4318e-01 -8.2686e-02 -2.6534e+00\n",
      " -4.7818e-01  1.8876e+00 -1.5444e+00  8.3629e-01 -1.6957e+00 -2.0685e-01\n",
      " -1.2084e+00  1.8173e+00 -1.0822e+00  1.3936e+00  5.0144e-01  1.6473e-01\n",
      "  6.3309e-01 -2.3186e+00  1.8351e-01 -5.3202e-01 -1.1665e+00 -3.8749e-01\n",
      " -1.6809e+00  1.3273e-01  1.3618e+00 -2.4581e-01 -1.1773e-01  6.5240e-01\n",
      "  2.2693e+00 -3.6834e-01 -1.4832e+00 -1.7459e+00 -1.2539e+00 -1.1275e+00]\n",
      "Tamanho do vetor: 300 dimensões.\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de vetor de uma palavra\n",
    "word = \"homen\"\n",
    "word_vector = nlp.vocab[word].vector\n",
    "\n",
    "print(f\"Vetor para a palavra '{word}':\\n{word_vector}\")\n",
    "print(f\"Tamanho do vetor: {len(word_vector)} dimensões.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff686c-8961-409d-abbf-0d1ebf680445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46cbbe57-e10c-4636-8f65-4eccf7533468",
   "metadata": {},
   "source": [
    "# Comparando Similaridade entre Palavras\n",
    "\n",
    "Uma das principais aplicações dos embeddings é calcular a similaridade entre palavras. Palavras com significados semelhantes terão vetores próximos no espaço vetorial.\n",
    "\n",
    "Vamos comparar a similaridade entre algumas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5760a4-4059-4ffa-9965-1288a04c8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = nlp(\"rei\")\n",
    "word2 = nlp(\"rainha\")\n",
    "word3 = nlp(\"homem\")\n",
    "word4 = nlp(\"mulher\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127e92f6-9c9e-4a54-84ff-8c8505918262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6001227943189619"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1.similarity(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b7ab3d0-d80a-4444-8208-89510444a162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4191260644255058"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1.similarity(word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3934be-b7f8-4d77-bd5b-5d326c9e54c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22795657259023788"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1.similarity(word4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b453c0b-e7d6-41c0-85bb-e3e4e1f16aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Similaridade entre 'rei' e 'rainha': {word1.similarity(word2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "804b97bc-6b7d-4059-b7eb-c1f1ca52b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade entre 'rei' e 'rainha': 0.6001\n",
      "Similaridade entre 'homem' e 'mulher': 0.6596\n",
      "Similaridade entre 'rei' e 'homem': 0.4191\n",
      "Similaridade entre 'rei' e 'mulher': 0.2280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Similaridade entre 'rei' e 'rainha': {word1.similarity(word2):.4f}\")\n",
    "print(f\"Similaridade entre 'homem' e 'mulher': {word3.similarity(word4):.4f}\")\n",
    "print(f\"Similaridade entre 'rei' e 'homem': {word1.similarity(word3):.4f}\")\n",
    "print(f\"Similaridade entre 'rei' e 'mulher': {word1.similarity(word4):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af2e341f-9f5a-4251-8ac5-4c84a185d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade entre as frases:\n",
      "'O rei governa o reino.'\n",
      "e\n",
      "'A rainha governa o império.': 0.6615\n"
     ]
    }
   ],
   "source": [
    "# Comparando similaridade entre frases\n",
    "sentence1 = nlp(\"O rei governa o reino.\")\n",
    "sentence2 = nlp(\"A rainha governa o império.\")\n",
    "\n",
    "similarity = sentence1.similarity(sentence2)\n",
    "print(f\"Similaridade entre as frases:\\n'{sentence1.text}'\\ne\\n'{sentence2.text}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadebc00-e1bf-46d8-a432-14e9d255ae9c",
   "metadata": {},
   "source": [
    "### Alterando o dicionário\n",
    "Embedding em portugues\n",
    "http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b0a9c4-9883-4aae-857d-7d485fc918b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "\n",
    "# Criar um vocabulário vazio\n",
    "vocab = Vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba7421-ee32-4c50-b511-6b1db0f60ce7",
   "metadata": {},
   "source": [
    "# Como a spaCy prevê similaridades?\n",
    "\n",
    "- A similaridade é determinada usando os **vetores de palavras**\n",
    "- Vetores são representações multi dimensionais das palavras\n",
    "- São gerados utilizando algoritmos similares a \n",
    "  [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) e uma enorme quantidade de textos.\n",
    "- Podem ser adicionados aos fluxos (pipelines) de processamento da spaCy.\n",
    "- Algoritmo padrão: similaridade por cosseno, mas pode ser alterado\n",
    "- Os vetores de `Doc` e `Span` são a média dos vetores de seus tokens.\n",
    "- Frases curtas são melhores que grandes documentos com palavras irrelevantes.\n",
    "\n",
    "Notes: Mas como a spaCy faz esse cálculo de similaridade?\n",
    "\n",
    "A similaridade é determinada utilizando-se vetores de palavras, que são representações\n",
    "multi dimensionais do significado de cada palavra.\n",
    "\n",
    "Você deve ter ouvido falar do Word2Vec, um algoritmo que é usado com frequencia para\n",
    "treinar vetores de palavras a partir de textos.\n",
    "\n",
    "Os vetores podem ser adicionados aos modelos estatísticos da spaCy.\n",
    "\n",
    "Por padrão, a similaridade calculada pela spaCy é a similaridade cosseno entre os\n",
    "dois vetores, mas isso pode ser alterado se necessário.\n",
    "\n",
    "O vetor de um objeto consistido de vários tokens, como o `Doc` e o `Span`, é calculado \n",
    "como a média dos vetores dos seus tokens.\n",
    "\n",
    "É por este motivo que você consegue extrair mais valor de frases curtas com poucas\n",
    "palavras irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf1802-c0c4-4e0a-ba85-c321175c75ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccc2b4-7dce-4034-a35c-f7115e2af401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
